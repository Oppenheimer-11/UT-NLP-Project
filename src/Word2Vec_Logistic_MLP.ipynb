{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "878baf36-6a21-4ce8-9b92-5064c2091a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "nltk.download('wordnet')\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.svm import SVC\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2920719f-b3fc-4398-80b7-61576ada4b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "file_path = r\"../datasets/\"\n",
    "train_file = r\"train.csv\"\n",
    "test_file = r\"test.csv\"\n",
    "subm_file = r\"sample_submission.csv\"\n",
    "\n",
    "train = pd.read_csv(file_path+train_file)\n",
    "test = pd.read_csv(file_path+test_file)\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47fccc9c-a3c4-4109-be8c-8ccb4c83e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "24556742-a429-4370-87e5-9871a1f17216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning sentences\n",
    "def clean_tweet(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags (#)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # Remove hashtags preceded by spaces\n",
    "    tweet = re.sub(r' #', ' ', tweet)\n",
    "    \n",
    "    # Remove dashes (-)\n",
    "    tweet = re.sub(r'-', '', tweet)\n",
    "    \n",
    "    # Remove special characters and punctuations (except alphanumeric and spaces)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    \n",
    "    # Remove non-ASCII characters and emojis\n",
    "    tweet = ''.join(char for char in tweet if char in string.printable)\n",
    "    \n",
    "    # Remove punctuation using Unicode categories\n",
    "    tweet = ''.join(char for char in tweet if not unicodedata.category(char).startswith('P'))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    \n",
    "    # Remove single characters (e.g., 'a', 'b', 'c')\n",
    "    tweet = re.sub(r'\\b\\w\\b', '', tweet)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    \n",
    "    # Remove the word \"nan\"\n",
    "    tweet = re.sub(r'\\bnan\\b', '', tweet)\n",
    "\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweet = ' '.join(word for word in tweet.split() if word not in stop_words)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def dataset_clean(train,test):\n",
    "    #Preprocessing\n",
    "    train['text_clean']=train.text.apply(clean_tweet)\n",
    "    test['text_clean']=test.text.apply(clean_tweet)\n",
    "    \n",
    "    #Lemmatization\n",
    "    lemma=WordNetLemmatizer()\n",
    "    train['text_lemma']=train['text_clean'].apply(preprocessdata)\n",
    "    test['text_lemma']=test['text_clean'].apply(preprocessdata)\n",
    "    \n",
    "    #tokenization\n",
    "    train['tokens']=train['text_lemma'].apply(lambda x: word_tokenize(x))\n",
    "    test['tokens'] = test['text_lemma'].apply(lambda x: word_tokenize(x))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a4478155-a7da-4636-a19d-8224ed979756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accurarcy(y_test, y_pred):\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "    print(f'precision: {precision * 100:.2f}%')\n",
    "    print(f'recall: {recall * 100:.2f}%')\n",
    "    \n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bf305c83-3525-449b-aae9-bd3f73d8f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>[resident, asked, shelter, place, notified, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1       deeds reason earthquake may allah forgive us   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  residents asked shelter place notified officer...   \n",
       "3       1  people receive wildfires evacuation orders cal...   \n",
       "4       1  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "                                          text_lemma  \\\n",
       "0         deed reason earthquake may allah forgive u   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident asked shelter place notified officer ...   \n",
       "3  people receive wildfire evacuation order calif...   \n",
       "4  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [deed, reason, earthquake, may, allah, forgive...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [resident, asked, shelter, place, notified, of...  \n",
       "3  [people, receive, wildfire, evacuation, order,...  \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test = dataset_clean(train,test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "02960fe3-8354-4b6d-9777-c2cdbe191eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(845988, 950780)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=pd.concat([train['tokens'],test['tokens']])\n",
    "#word2vec using skip gram\n",
    "w2v_model = Word2Vec(corpus,vector_size=150,window=7,min_count=2,sg=1)\n",
    "w2v_model.train(corpus,total_examples=len(corpus),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "994de61f-2f40-4c26-8dbc-74fd22d17be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create document vectors by averaging word vectors. Remove out-of-vocabulary words\n",
    "def get_word_embeddings(token_list,vector,k=150):\n",
    "    if len(token_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    else:\n",
    "        vectorized = [vector.wv[word] if word in vector.wv else np.random.rand(k) for word in token_list] \n",
    "    \n",
    "    sum = np.sum(vectorized,axis=0)\n",
    "    ## return the average\n",
    "    return sum/len(vectorized)     \n",
    "\n",
    "def get_embeddings(tokens,vector):\n",
    "        embeddings = tokens.apply(lambda x: get_word_embeddings(x, w2v_model))\n",
    "        return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30cfc400-6e6d-4fb6-87cb-461eee9865d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =get_embeddings(train['tokens'],w2v_model)\n",
    "y=train.target.values\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cd084-01e4-4e20-8c06-bb4e235d48d0",
   "metadata": {},
   "source": [
    "### Using Naive Bayes with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "76d184c8-044a-40f5-9e7f-3378a28cb0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 73.11%\n",
      "precision: 73.28%\n",
      "recall: 72.99%\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78      1318\n",
      "           1       0.70      0.67      0.68       966\n",
      "\n",
      "    accuracy                           0.74      2284\n",
      "   macro avg       0.73      0.73      0.73      2284\n",
      "weighted avg       0.74      0.74      0.74      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using Naive Bayes\n",
    "nb_classifier = MultinomialNB()\n",
    "X_train_nb = np.array(X_train)\n",
    "X_test_nb = np.array(X_test)\n",
    "X_train_binary = np.where(X_train_nb > 0, 1, 0)\n",
    "X_test_binary = np.where(X_test_nb> 0, 1, 0)\n",
    "\n",
    "nb_classifier.fit(X_train_binary, y_train)\n",
    "y_pred_nb = nb_classifier.predict(X_test_binary)\n",
    "\n",
    "accurarcy(y_test,y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595ce56-77e8-493a-a1b3-b21b1dad91b0",
   "metadata": {},
   "source": [
    "### Using SVM with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0f72a90a-3e17-43b0-a21b-d8273ceb1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 76.90%\n",
      "precision: 78.25%\n",
      "recall: 76.42%\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82      1318\n",
      "           1       0.79      0.66      0.72       966\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.78      0.76      0.77      2284\n",
      "weighted avg       0.78      0.78      0.78      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using SVM\n",
    "svm_model=SVC(kernel=\"linear\", probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "accurarcy(y_test,y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a5c9ee-992f-49b0-8737-8e1e4fe7c3fd",
   "metadata": {},
   "source": [
    "### Using Logistic Regression with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0202dd58-c0f5-4f13-bea8-c3fa0f51b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create logistic regression\n",
    "lr_model=LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b556f38f-26f9-4eb0-beba-a4edea451bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 76.88%\n",
      "Logistic Classification with Word2Vec Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82      1318\n",
      "           1       0.78      0.66      0.72       966\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.78      0.76      0.77      2284\n",
      "weighted avg       0.78      0.78      0.78      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "\n",
    "report_lg = classification_report(y_test, y_pred)\n",
    "print(\"Logistic Classification with Word2Vec Report:\\n\", report_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e2d85-3cfb-42cf-b35b-b8b68cde0f2c",
   "metadata": {},
   "source": [
    "### Using MLP with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c57bec85-8605-40ea-a8b0-ddee2f784448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(np.array(y_train, dtype=np.float32)).view(-1, 1).to(device)  # Ensure labels are float and have a 2nd dimension\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(np.array(y_test, dtype=np.float32)).view(-1, 1).to(device)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d3871c7e-2967-405c-af2f-d47abe754cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the MLP model\n",
    "input_dim = w2v_model.vector_size\n",
    "hidden_dim = 128  # You can adjust the number of hidden units\n",
    "output_dim = 1  # For binary classification\n",
    "model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "665d3b1d-5479-42c9-bc42-2e5d1357c76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 77.73%\n",
      "precision: 79.41%\n",
      "recall: 77.18%\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.89      0.83      1318\n",
      "         1.0       0.81      0.66      0.73       966\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.77      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5  # Adjust the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predicted = (outputs >= 0.5).float().view(-1, 1)  # Adjust threshold for binary classification\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    # print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    \n",
    "    predicted_binary = (outputs >= 0.5).float().view(-1, 1)\n",
    "    y_test_binary = (y_test_tensor >= 0.5).float().view(-1, 1)\n",
    "    accurarcy(y_test_binary,predicted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b4fce-4aa0-4d38-8838-b2132633b165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

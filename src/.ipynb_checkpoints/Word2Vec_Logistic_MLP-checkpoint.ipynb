{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878baf36-6a21-4ce8-9b92-5064c2091a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.svm import SVC\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2920719f-b3fc-4398-80b7-61576ada4b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "file_path = r\"../datasets/\"\n",
    "train_file = r\"train.csv\"\n",
    "test_file = r\"test.csv\"\n",
    "subm_file = r\"sample_submission.csv\"\n",
    "\n",
    "train = pd.read_csv(file_path+train_file)\n",
    "test = pd.read_csv(file_path+test_file)\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47fccc9c-a3c4-4109-be8c-8ccb4c83e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24556742-a429-4370-87e5-9871a1f17216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning sentences\n",
    "def clean_tweet(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags (#)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # Remove hashtags preceded by spaces\n",
    "    tweet = re.sub(r' #', ' ', tweet)\n",
    "    \n",
    "    # Remove dashes (-)\n",
    "    tweet = re.sub(r'-', '', tweet)\n",
    "    \n",
    "    # Remove special characters and punctuations (except alphanumeric and spaces)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    \n",
    "    # Remove non-ASCII characters and emojis\n",
    "    tweet = ''.join(char for char in tweet if char in string.printable)\n",
    "    \n",
    "    # Remove punctuation using Unicode categories\n",
    "    tweet = ''.join(char for char in tweet if not unicodedata.category(char).startswith('P'))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    \n",
    "    # Remove single characters (e.g., 'a', 'b', 'c')\n",
    "    tweet = re.sub(r'\\b\\w\\b', '', tweet)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    \n",
    "    # Remove the word \"nan\"\n",
    "    tweet = re.sub(r'\\bnan\\b', '', tweet)\n",
    "\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweet = ' '.join(word for word in tweet.split() if word not in stop_words)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    # 将词性标签映射到WordNet词性标签\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "    return tag\n",
    "\n",
    "def dataset_clean(train,test):\n",
    "    #Preprocessing\n",
    "    train['text_clean']=train.text.apply(clean_tweet)\n",
    "    test['text_clean']=test.text.apply(clean_tweet)\n",
    "    \n",
    "    #Lemmatization\n",
    "    lemma=WordNetLemmatizer()\n",
    "    train['text_lemma']=train['text_clean'].apply(lambda x: ' '.join(lemma.lemmatize(word, get_wordnet_pos(word)) for word in x.split()))\n",
    "    test['text_lemma']=test['text_clean'].apply(lambda x: ' '.join(lemma.lemmatize(word, get_wordnet_pos(word)) for word in x.split()))\n",
    "    \n",
    "    #tokenization\n",
    "    train['tokens']=train['text_lemma'].apply(lambda x: word_tokenize(x))\n",
    "    test['tokens'] = test['text_lemma'].apply(lambda x: word_tokenize(x))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4478155-a7da-4636-a19d-8224ed979756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accurarcy(y_test, y_pred):\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "    print(f'precision: {precision * 100:.2f}%')\n",
    "    print(f'recall: {recall * 100:.2f}%')\n",
    "    \n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf305c83-3525-449b-aae9-bd3f73d8f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>[get, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1       deeds reason earthquake may allah forgive us   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  residents asked shelter place notified officer...   \n",
       "3       1  people receive wildfires evacuation orders cal...   \n",
       "4       1  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "                                          text_lemma  \\\n",
       "0         deed reason earthquake may allah forgive u   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3  people receive wildfire evacuation order calif...   \n",
       "4  get sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [deed, reason, earthquake, may, allah, forgive...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [resident, ask, shelter, place, notify, office...  \n",
       "3  [people, receive, wildfire, evacuation, order,...  \n",
       "4  [get, sent, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test = dataset_clean(train,test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02960fe3-8354-4b6d-9777-c2cdbe191eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847307, 950780)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=pd.concat([train['tokens'],test['tokens']])\n",
    "#word2vec using skip gram\n",
    "w2v_model = Word2Vec(corpus,vector_size=150,window=7,min_count=2,sg=1)\n",
    "w2v_model.train(corpus,total_examples=len(corpus),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994de61f-2f40-4c26-8dbc-74fd22d17be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create document vectors by averaging word vectors. Remove out-of-vocabulary words\n",
    "def get_word_embeddings(token_list,vector,k=150):\n",
    "    if len(token_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    else:\n",
    "        vectorized = [vector.wv[word] if word in vector.wv else np.random.rand(k) for word in token_list] \n",
    "    \n",
    "    sum = np.sum(vectorized,axis=0)\n",
    "    ## return the average\n",
    "    return sum/len(vectorized)     \n",
    "\n",
    "def get_embeddings(tokens,vector):\n",
    "        embeddings = tokens.apply(lambda x: get_word_embeddings(x, w2v_model))\n",
    "        return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30cfc400-6e6d-4fb6-87cb-461eee9865d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =get_embeddings(train['tokens'],w2v_model)\n",
    "y=train.target.values\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "val_data, X_test, val_labels, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cd084-01e4-4e20-8c06-bb4e235d48d0",
   "metadata": {},
   "source": [
    "### Using Naive Bayes with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d184c8-044a-40f5-9e7f-3378a28cb0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 74.34%\n",
      "precision: 74.57%\n",
      "recall: 74.18%\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.79      1318\n",
      "           1       0.72      0.68      0.70       966\n",
      "\n",
      "    accuracy                           0.75      2284\n",
      "   macro avg       0.75      0.74      0.74      2284\n",
      "weighted avg       0.75      0.75      0.75      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using Naive Bayes\n",
    "nb_classifier = MultinomialNB()\n",
    "X_train_nb = np.array(X_train)\n",
    "X_test_nb = np.array(X_test)\n",
    "X_train_binary = np.where(X_train_nb > 0, 1, 0)\n",
    "X_test_binary = np.where(X_test_nb> 0, 1, 0)\n",
    "\n",
    "nb_classifier.fit(X_train_binary, y_train)\n",
    "y_pred_nb = nb_classifier.predict(X_test_binary)\n",
    "\n",
    "accurarcy(y_test,y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2626e1f0-58c6-46d1-8243-3042f10f6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1059  259]\n",
      " [ 309  657]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595ce56-77e8-493a-a1b3-b21b1dad91b0",
   "metadata": {},
   "source": [
    "### Using SVM with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f72a90a-3e17-43b0-a21b-d8273ceb1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 77.34%\n",
      "precision: 78.92%\n",
      "recall: 76.81%\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83      1318\n",
      "           1       0.80      0.66      0.72       966\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.77      0.77      2284\n",
      "weighted avg       0.79      0.79      0.78      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using SVM\n",
    "svm_model=SVC(kernel=\"linear\", probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "accurarcy(y_test,y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a5c9ee-992f-49b0-8737-8e1e4fe7c3fd",
   "metadata": {},
   "source": [
    "### Using Logistic Regression with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0202dd58-c0f5-4f13-bea8-c3fa0f51b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create logistic regression\n",
    "lr_model=LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b556f38f-26f9-4eb0-beba-a4edea451bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 77.14%\n",
      "Logistic Classification with Word2Vec Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82      1318\n",
      "           1       0.78      0.67      0.72       966\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.78      0.77      0.77      2284\n",
      "weighted avg       0.78      0.78      0.78      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "\n",
    "report_lg = classification_report(y_test, y_pred)\n",
    "print(\"Logistic Classification with Word2Vec Report:\\n\", report_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e2d85-3cfb-42cf-b35b-b8b68cde0f2c",
   "metadata": {},
   "source": [
    "### Using MLP with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c57bec85-8605-40ea-a8b0-ddee2f784448",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_140/1330839178.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  X_train_tensor = torch.FloatTensor(X_train).to(device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(np.array(y_train, dtype=np.float32)).view(-1, 1).to(device)  # Ensure labels are float and have a 2nd dimension\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(np.array(y_test, dtype=np.float32)).view(-1, 1).to(device)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3871c7e-2967-405c-af2f-d47abe754cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/nlp_project1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Define a custom MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the MLP model\n",
    "input_dim = w2v_model.vector_size\n",
    "hidden_dim = 128  # You can adjust the number of hidden units\n",
    "output_dim = 1  # For binary classification\n",
    "model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "665d3b1d-5479-42c9-bc42-2e5d1357c76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/nlp_project1/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 77.21%\n",
      "precision: 77.50%\n",
      "recall: 77.01%\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.83      0.81      1318\n",
      "         1.0       0.75      0.71      0.73       966\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.78      0.77      0.77      2284\n",
      "weighted avg       0.78      0.78      0.78      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5  # Adjust the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predicted = (outputs >= 0.5).float().view(-1, 1)  # Adjust threshold for binary classification\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    # print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    \n",
    "    predicted_binary = (outputs >= 0.5).float().view(-1, 1)\n",
    "    y_test_binary = (y_test_tensor >= 0.5).float().view(-1, 1)\n",
    "    accurarcy(y_test_binary,predicted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b4fce-4aa0-4d38-8838-b2132633b165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
